{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "FWLAwxHIdWvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
        "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "data = pd.read_csv(url, sep='\\t', names=columns)\n",
        "\n",
        "# Create user-item matrix\n",
        "matrix = data.pivot_table(index='item_id', columns='user_id', values='rating')\n"
      ],
      "metadata": {
        "id": "GsrO-inkv0Ld"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item based"
      ],
      "metadata": {
        "id": "66Mx_1bAdcpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "n_splits = 5\n",
        "\n",
        "K_values = [10, 20, 30, 40, 50]\n",
        "\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "mae_values = []\n",
        "for train_index, test_index in kf.split(matrix):\n",
        "    \n",
        "    train_data = matrix.iloc[train_index].fillna(0)\n",
        "    test_data = matrix.iloc[test_index].fillna(0)\n",
        "    \n",
        "    for K in K_values:\n",
        "        predicted_ratings = []\n",
        "        for item_id, test_ratings in test_data.iterrows():\n",
        "            \n",
        "            # Compute the cosine similarity between the test item and all other items\n",
        "            \n",
        "            similarities = cosine_similarity([test_ratings], train_data, dense_output=True)\n",
        "\n",
        "            neighbor_indices = similarities.argsort()[0][-K:]\n",
        "\n",
        "            neighbor_ratings = train_data.iloc[neighbor_indices]\n",
        "\n",
        "            weights = similarities[0][neighbor_indices]\n",
        "            weighted_ratings = np.multiply(neighbor_ratings, weights[:, np.newaxis])\n",
        "            predicted_rating = np.sum(weighted_ratings) / np.sum(weights)\n",
        "\n",
        "            predicted_ratings.append(predicted_rating)\n",
        "\n",
        "        # Compute MAE \n",
        "        mae = np.abs(test_data.values - predicted_ratings).mean()\n",
        "        mae_values.append(mae)\n",
        "        print(f\"Item-based, K={K}: MAE = {mae}\")\n",
        "\n",
        "        # Add the MAE to the list\n",
        "\n"
      ],
      "metadata": {
        "id": "8EsdQMaJyFub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance Weighting\n"
      ],
      "metadata": {
        "id": "1G0ct0KYND7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# folds\n",
        "n_splits = 5\n",
        "\n",
        "K_values = [10, 20, 30, 40, 50]\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
        "\n",
        "# stores MAE for each K\n",
        "mae_values_1 = []\n",
        "\n",
        "for train_index, test_index in kf.split(matrix):\n",
        "    \n",
        "    #training and testing splits\n",
        "    train_data = matrix.iloc[train_index].fillna(0)\n",
        "    test_data = matrix.iloc[test_index].fillna(0)\n",
        "    \n",
        "    for K in K_values:\n",
        "\n",
        "        predicted_ratings_1 = []\n",
        "\n",
        "        for item_id, test_ratings in test_data.iterrows():       #test items\n",
        "            \n",
        "            \n",
        "\n",
        "            # Compute the cosine similarity between the test item and all other items\n",
        "            similarities = cosine_similarity([test_ratings], train_data, dense_output=True)\n",
        "\n",
        "            # indexes of K nearest neighbours\n",
        "            neighbor_indices_1 = similarities.argsort()[0][-K:]\n",
        "\n",
        "            # ratings of neighbours\n",
        "            neighbor_ratings_1 = train_data.iloc[neighbor_indices_1]\n",
        "\n",
        "            # Compute the variance of the ratings of the K nearest neighbors\n",
        "            variances = np.var(neighbor_ratings_1, axis=0)\n",
        "\n",
        "            # Compute the weights as the inverse of the variances\n",
        "            eps1 = 1e-6\n",
        "            s=1e-6+0.8\n",
        "            weights_1 = 1.0 / (variances + eps1)    #eps avoids division by zero\n",
        "\n",
        "            # predicted rating = weighted average of the neighbor ratings\n",
        "            # weights_1 = similarities[0][neighbor_indices_1]\n",
        "\n",
        "            weighted_ratings_1 = np.multiply(neighbor_ratings_1, weights_1)\n",
        "\n",
        "            predicted_rating_1 = np.sum(weighted_ratings_1) / np.sum(weights_1)\n",
        "\n",
        "            predicted_ratings_1.append(predicted_rating_1)\n",
        "\n",
        "        # Compute the MAE for this value of K\n",
        "\n",
        "        mae_1 = np.abs(test_data.values - predicted_ratings_1).mean()\n",
        "\n",
        "        # Print the MAE\n",
        "        print(f\"Item-based, K={K}: MAE = {mae_1}\")\n",
        "\n",
        "        # Add the MAE to the list\n",
        "        mae_values_1.append(mae_1)\n",
        "\n"
      ],
      "metadata": {
        "id": "ruIgGc_OleaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}